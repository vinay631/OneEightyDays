{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "dayOfWeekReviews = defaultdict(dict)\n",
    "with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        date = datetime.datetime.strptime(jsonData['date'], '%Y-%m-%d')\n",
    "        star = jsonData['stars']\n",
    "        weekDay = date.weekday()\n",
    "        \n",
    "        weekDayStars = dayOfWeekReviews[weekDay]\n",
    "        if star in weekDayStars:\n",
    "            weekDayStars[star] += 1\n",
    "        else:\n",
    "            weekDayStars[star] = 1\n",
    "            \n",
    "        dayOfWeekReviews[weekDay] = weekDayStars\n",
    "\n",
    "for k, v in dayOfWeekReviews.iteritems():\n",
    "    print k, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(7):\n",
    "    d = dayOfWeekReviews[i]\n",
    "    print d[1], d[2], d[3], d[4], d[5], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "businessTypes = defaultdict(int)\n",
    "businessCountByCity = defaultdict(int)\n",
    "businessTypeCountByCity = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "with open('./data/yelp_academic_dataset_business.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        city = jsonData['city']\n",
    "        state = jsonData['state']\n",
    "        categories = jsonData['categories']\n",
    "        for cat in categories:\n",
    "            businessTypes[cat] += 1\n",
    "            businessTypeCountByCity[(city, state)][cat] += 1\n",
    "            \n",
    "        businessCountByCity[(city, state)] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(businessCountByCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for k, v in businessCountByCity.iteritems():\n",
    "#    print k[0], '|', k[1], '|', v\n",
    "import operator\n",
    "\n",
    "print max(businessCountByCity.iteritems(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = 1\n",
    "dailyIncrease = .18/100\n",
    "for i in xrange(36):\n",
    "    value += value * dailyIncrease\n",
    "\n",
    "print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewLengthByStar = defaultdict(list)\n",
    "numOfStarsByUid = defaultdict(lambda : defaultdict(int))\n",
    "with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        star = jsonData['stars']\n",
    "        weekDay = date.weekday()\n",
    "        text = jsonData['text']\n",
    "        userId = jsonData['user_id']\n",
    "        reviewLengthByStar[star].append(len(text))\n",
    "        numOfStarsByUid[userId][star] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for star in reviewLengthByStar:\n",
    "    reviewLengths = reviewLengthByStar[star]\n",
    "    print star, np.mean(reviewLengths), np.std(reviewLengths), len(reviewLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cat, count in businessTypes.iteritems():\n",
    "    print cat, '|', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import toyplot.color\n",
    "toyplot.color.Palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "observations = np.random.normal(size=(50, 50))\n",
    "\n",
    "x = np.linspace(0, 1, len(observations))\n",
    "y1 = np.min(observations, axis=1)\n",
    "y2 = np.max(observations, axis=1)\n",
    "\n",
    "canvas = toyplot.Canvas(width=400, height=300)\n",
    "axes = canvas.axes()\n",
    "mark = axes.fill(x, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.yelp_dataset\n",
    "business = db.business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usBoundingBox = ((-124.848974, 24.396308), (-66.885444, 49.384358))\n",
    "businessCursor = business.find({'categories':'Restaurants', \n",
    "                                'longitude': {'$lte': -66.885444}, \n",
    "                                'longitude': {'$gte': -124.848974}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allBusiness = [b for b in businessCursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21892\n",
      "{u'city': u'Homestead', u'review_count': 3, u'name': u\"Eat'n Park Hospitality Group\", u'neighborhoods': [u'Homestead'], u'open': True, u'business_id': u'sRqB6flj3GtTZIZJQxf_oA', u'full_address': u'285 Waterfront Dr E\\nHomestead\\nHomestead, PA 15120', u'hours': {}, u'state': u'PA', u'longitude': -79.9123428, u'stars': 2.5, u'latitude': 40.4116918, u'attributes': {u'Noise Level': u'very_loud', u'Parking': {u'garage': False, u'street': False, u'validated': False, u'lot': False, u'valet': False}, u'Alcohol': u'beer_and_wine', u'Attire': u'casual'}, u'_id': ObjectId('55d1dfe9fa8def3669b5fcf5'), u'type': u'business', u'categories': [u'Restaurants']}\n"
     ]
    }
   ],
   "source": [
    "print len(allBusiness)\n",
    "print allBusiness[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537223 RdFot8wgSaCtPUatrAkiqA\n"
     ]
    }
   ],
   "source": [
    "reviews = db.reviews\n",
    "tags = db.tags\n",
    "tagCursor = tags.find()\n",
    "tagged_reviews = [t['review_id'] for t in tagCursor]\n",
    "print len(tagged_reviews), tagged_reviews[0]\n",
    "businessIds = [b['business_id'] for b in allBusiness]\n",
    "reviewCur = reviews.find({'business_id':{'$in':businessIds}})\n",
    "usRestaurantReviews = [r for r in reviewCur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990627\n",
      "{u'votes': {u'funny': 1, u'useful': 1, u'cool': 1}, u'user_id': u'DBz7nWHg6tLK1JzLFOtU3A', u'review_id': u'RdFot8wgSaCtPUatrAkiqA', u'text': u'Great guys, great food. Plus right next to one of my favorite bars..', u'business_id': u'--5jkZ3-nUPZxUvtcbr8Uw', u'stars': 5, u'date': datetime.datetime(2011, 7, 31, 0, 0), u'_id': ObjectId('55d1de3bfa8def3669b0fb65'), u'type': u'review'}\n",
      "453404\n"
     ]
    }
   ],
   "source": [
    "print len(usRestaurantReviews)\n",
    "print usRestaurantReviews[0]\n",
    "allRestReviews = [r['review_id'] for r in usRestaurantReviews]\n",
    "notTagged = list(set(allRestReviews) - set(tagged_reviews))\n",
    "print len(notTagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#notTaggedReviews = list(notTagged)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import string\n",
    "\n",
    "def worker(id, db, skip, limit):\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append(\"''\")\n",
    "    reviewColls = db.reviews\n",
    "    tagColls = db.tmptags\n",
    "    batchSize = 100\n",
    "    #for batch in xrange(0, limit, batchSize):\n",
    "    if True:\n",
    "        reviewCur = reviewColls.find({'review_id':{'$in':notTagged}}).skip(skip).limit(limit)\n",
    "        bulkInsertData = []\n",
    "        for review in reviewCur:\n",
    "            words = []\n",
    "            reviewText = review['text'].lower()\n",
    "            businessId = review['business_id']\n",
    "            userId = review['user_id']\n",
    "            date = review['date']\n",
    "            reviewId = review['review_id']\n",
    "            stars = review['stars']\n",
    "            sentences = nltk.sent_tokenize(reviewText)\n",
    "            for sentence in sentences:\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tokens = [t for t in tokens if t not in punctuations]\n",
    "                text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                for word, tag in tagged_text:\n",
    "                    words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "                insertData = {'business_id': businessId,\n",
    "                              'user_id': userId,\n",
    "                              'date': date,\n",
    "                              'review_id': reviewId,\n",
    "                              'stars': stars,\n",
    "                              'words': words}\n",
    "            #tagColls.insert(insertData)\n",
    "        bulkInsertData.append(insertData)\n",
    "        tagColls.insert_many(bulkInsertData)\n",
    "#worker(1, db, 0, 0)\n",
    "numWorkers = 2\n",
    "#print notTagged[:50]\n",
    "reviewCur = reviews.find({'review_id':{'$in':notTagged}})\n",
    "totalReviews = reviewCur.count()\n",
    "batchSize = totalReviews/numWorkers\n",
    "extra = totalReviews % numWorkers\n",
    "\n",
    "jobs = []\n",
    "for i in xrange(numWorkers):\n",
    "    left = 0\n",
    "    if i == (numWorkers - 1):\n",
    "        left = extra\n",
    "    p = multiprocessing.Process(target=worker, args=((i + 1), db, i * batchSize, batchSize + left))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "\n",
    "for j in jobs:\n",
    "    j.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def create_corpus():\n",
    "    lm = WordNetLemmatizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews 453404\n",
      "10 20 30"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e99444292372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_json_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e99444292372>\u001b[0m in \u001b[0;36mcreate_json_array\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmandal/anaconda/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m    101\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 102\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmandal/anaconda/lib/python2.7/site-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def create_json_array():\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append(\"''\")\n",
    "    reviewColls = db.reviews\n",
    "    tagColls = db.tmptags\n",
    "    batchSize = 100\n",
    "    bulkInsertData = []\n",
    "    \n",
    "    \n",
    "    #for batch in xrange(0, limit, batchSize):\n",
    "    if True:\n",
    "        reviewCur = reviewColls.find({'review_id':{'$in':notTagged}})\n",
    "        count = 0\n",
    "        totalReviews = reviewCur.count()\n",
    "        print \"Total reviews\", totalReviews\n",
    "        percentComplete = 0\n",
    "        tenPercentCount = totalReviews/10\n",
    "        for review in reviewCur:\n",
    "            count += 1\n",
    "            words = []\n",
    "            reviewText = review['text'].lower()\n",
    "            businessId = review['business_id']\n",
    "            userId = review['user_id']\n",
    "            date = review['date']\n",
    "            reviewId = review['review_id']\n",
    "            stars = review['stars']\n",
    "            sentences = nltk.sent_tokenize(reviewText)\n",
    "            for sentence in sentences:\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tokens = [t for t in tokens if t not in punctuations]\n",
    "                text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                for word, tag in tagged_text:\n",
    "                    words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "            insertData = {'business_id': businessId,\n",
    "                              'user_id': userId,\n",
    "                              'date': date,\n",
    "                              'review_id': reviewId,\n",
    "                              'stars': stars,\n",
    "                              'words': words}\n",
    "            #tagColls.insert(insertData)\n",
    "            bulkInsertData.append(insertData)\n",
    "            if count == tenPercentCount:\n",
    "                percentComplete += 10\n",
    "                print percentComplete,\n",
    "                count = 0\n",
    "        dump = json.dumps(bulkInsertData)\n",
    "        return dump\n",
    "        \n",
    "dump = create_json_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-58f0f62021fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/derivedData/tokenized.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/yelp_academic_dataset_review.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mjsonData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mreviewId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "punctuations.append(\"''\")\n",
    "bulkInsertData = []\n",
    "tagColls = db.tmptags\n",
    "with open('./data/derivedData/tokenized.json', 'a') as fwrite:\n",
    "    with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "        for line in fread:\n",
    "            jsonData = json.loads(line)\n",
    "            reviewId = jsonData['review_id']\n",
    "            if reviewId in notTagged:\n",
    "                words = []\n",
    "                reviewText = jsonData['text'].lower()\n",
    "                businessId = jsonData['business_id']\n",
    "                userId = jsonData['user_id']\n",
    "                date = jsonData['date']\n",
    "                stars = jsonData['stars']\n",
    "                sentences = nltk.sent_tokenize(reviewText)\n",
    "                for sentence in sentences:\n",
    "                    tokens = nltk.word_tokenize(sentence)\n",
    "                    tokens = [t for t in tokens if t not in punctuations]\n",
    "                    text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                    tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                    for word, tag in tagged_text:\n",
    "                        words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "                insertData = {'business_id': businessId,\n",
    "                                  'user_id': userId,\n",
    "                                  'date': date,\n",
    "                                  'review_id': reviewId,\n",
    "                                  'stars': stars,\n",
    "                                  'words': words}\n",
    "                json.dump(insertData, fwrite)\n",
    "                fwrite.write('\\n')\n",
    "            #bulkInsertData.append(insertData)\n",
    "#tagColls.insert_many(bulkInsertData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
