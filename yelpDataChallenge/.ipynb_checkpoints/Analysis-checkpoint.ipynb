{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "dayOfWeekReviews = defaultdict(dict)\n",
    "with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        date = datetime.datetime.strptime(jsonData['date'], '%Y-%m-%d')\n",
    "        star = jsonData['stars']\n",
    "        weekDay = date.weekday()\n",
    "        \n",
    "        weekDayStars = dayOfWeekReviews[weekDay]\n",
    "        if star in weekDayStars:\n",
    "            weekDayStars[star] += 1\n",
    "        else:\n",
    "            weekDayStars[star] = 1\n",
    "            \n",
    "        dayOfWeekReviews[weekDay] = weekDayStars\n",
    "\n",
    "for k, v in dayOfWeekReviews.iteritems():\n",
    "    print k, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(7):\n",
    "    d = dayOfWeekReviews[i]\n",
    "    print d[1], d[2], d[3], d[4], d[5], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'city': u'Phoenix', u'review_count': 9, u'name': u'Eric Goldberg, MD', u'neighborhoods': [], u'type': u'business', u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'full_address': u'4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018', u'hours': {u'Thursday': {u'close': u'17:00', u'open': u'08:00'}, u'Tuesday': {u'close': u'17:00', u'open': u'08:00'}, u'Friday': {u'close': u'17:00', u'open': u'08:00'}, u'Wednesday': {u'close': u'17:00', u'open': u'08:00'}, u'Monday': {u'close': u'17:00', u'open': u'08:00'}}, u'state': u'AZ', u'longitude': -111.983758, u'stars': 3.5, u'latitude': 33.499313, u'attributes': {u'By Appointment Only': True}, u'open': True, u'categories': [u'Doctors', u'Health & Medical']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "businessTypes = defaultdict(int)\n",
    "businessCountByCity = defaultdict(int)\n",
    "businessTypeCountByCity = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "with open('./data/yelp_academic_dataset_business.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        print jsonData\n",
    "        city = jsonData['city']\n",
    "        state = jsonData['state']\n",
    "        categories = jsonData['categories']\n",
    "        for cat in categories:\n",
    "            businessTypes[cat] += 1\n",
    "            businessTypeCountByCity[(city, state)][cat] += 1\n",
    "            \n",
    "        businessCountByCity[(city, state)] += 1\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "print len(businessCountByCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in businessCountByCity.iteritems():\n",
    "    print k[0], '|', k[1], '|', v\n",
    "#import operator\n",
    "\n",
    "#print max(businessCountByCity.iteritems(), key=operator.itemgetter(1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = 1\n",
    "dailyIncrease = .18/100\n",
    "for i in xrange(36):\n",
    "    value += value * dailyIncrease\n",
    "\n",
    "print value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewLengthByStar = defaultdict(list)\n",
    "numOfStarsByUid = defaultdict(lambda : defaultdict(int))\n",
    "with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        star = jsonData['stars']\n",
    "        weekDay = date.weekday()\n",
    "        text = jsonData['text']\n",
    "        userId = jsonData['user_id']\n",
    "        reviewLengthByStar[star].append(len(text))\n",
    "        numOfStarsByUid[userId][star] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for star in reviewLengthByStar:\n",
    "    reviewLengths = reviewLengthByStar[star]\n",
    "    print star, np.mean(reviewLengths), np.std(reviewLengths), len(reviewLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cat, count in businessTypes.iteritems():\n",
    "    print cat, '|', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import toyplot.color\n",
    "toyplot.color.Palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "observations = np.random.normal(size=(50, 50))\n",
    "\n",
    "x = np.linspace(0, 1, len(observations))\n",
    "y1 = np.min(observations, axis=1)\n",
    "y2 = np.max(observations, axis=1)\n",
    "\n",
    "canvas = toyplot.Canvas(width=400, height=300)\n",
    "axes = canvas.axes()\n",
    "mark = axes.fill(x, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.yelp_dataset\n",
    "business = db.business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usBoundingBox = ((-124.848974, 24.396308), (-66.885444, 49.384358))\n",
    "businessCursor = business.find({'categories':'Restaurants', \n",
    "                                'longitude': {'$lte': -66.885444}, \n",
    "                                'longitude': {'$gte': -124.848974}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allBusiness = [b for b in businessCursor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21892\n",
      "{u'city': u'Homestead', u'review_count': 3, u'name': u\"Eat'n Park Hospitality Group\", u'neighborhoods': [u'Homestead'], u'open': True, u'business_id': u'sRqB6flj3GtTZIZJQxf_oA', u'full_address': u'285 Waterfront Dr E\\nHomestead\\nHomestead, PA 15120', u'hours': {}, u'state': u'PA', u'longitude': -79.9123428, u'stars': 2.5, u'latitude': 40.4116918, u'attributes': {u'Noise Level': u'very_loud', u'Parking': {u'garage': False, u'street': False, u'validated': False, u'lot': False, u'valet': False}, u'Alcohol': u'beer_and_wine', u'Attire': u'casual'}, u'_id': ObjectId('55d1dfe9fa8def3669b5fcf5'), u'type': u'business', u'categories': [u'Restaurants']}\n"
     ]
    }
   ],
   "source": [
    "print len(allBusiness)\n",
    "print allBusiness[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537223 RdFot8wgSaCtPUatrAkiqA\n"
     ]
    }
   ],
   "source": [
    "reviews = db.reviews\n",
    "tags = db.tags\n",
    "tagCursor = tags.find()\n",
    "tagged_reviews = [t['review_id'] for t in tagCursor]\n",
    "print len(tagged_reviews), tagged_reviews[0]\n",
    "businessIds = [b['business_id'] for b in allBusiness]\n",
    "reviewCur = reviews.find({'business_id':{'$in':businessIds}})\n",
    "usRestaurantReviews = [r for r in reviewCur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990627\n",
      "{u'votes': {u'funny': 1, u'useful': 1, u'cool': 1}, u'user_id': u'DBz7nWHg6tLK1JzLFOtU3A', u'review_id': u'RdFot8wgSaCtPUatrAkiqA', u'text': u'Great guys, great food. Plus right next to one of my favorite bars..', u'business_id': u'--5jkZ3-nUPZxUvtcbr8Uw', u'stars': 5, u'date': datetime.datetime(2011, 7, 31, 0, 0), u'_id': ObjectId('55d1de3bfa8def3669b0fb65'), u'type': u'review'}\n",
      "453404\n"
     ]
    }
   ],
   "source": [
    "print len(usRestaurantReviews)\n",
    "print usRestaurantReviews[0]\n",
    "allRestReviews = [r['review_id'] for r in usRestaurantReviews]\n",
    "notTagged = list(set(allRestReviews) - set(tagged_reviews))\n",
    "print len(notTagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#notTaggedReviews = list(notTagged)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import string\n",
    "\n",
    "def worker(id, db, skip, limit):\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append(\"''\")\n",
    "    reviewColls = db.reviews\n",
    "    tagColls = db.tmptags\n",
    "    batchSize = 100\n",
    "    #for batch in xrange(0, limit, batchSize):\n",
    "    if True:\n",
    "        reviewCur = reviewColls.find({'review_id':{'$in':notTagged}}).skip(skip).limit(limit)\n",
    "        bulkInsertData = []\n",
    "        for review in reviewCur:\n",
    "            words = []\n",
    "            reviewText = review['text'].lower()\n",
    "            businessId = review['business_id']\n",
    "            userId = review['user_id']\n",
    "            date = review['date']\n",
    "            reviewId = review['review_id']\n",
    "            stars = review['stars']\n",
    "            sentences = nltk.sent_tokenize(reviewText)\n",
    "            for sentence in sentences:\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tokens = [t for t in tokens if t not in punctuations]\n",
    "                text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                for word, tag in tagged_text:\n",
    "                    words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "                insertData = {'business_id': businessId,\n",
    "                              'user_id': userId,\n",
    "                              'date': date,\n",
    "                              'review_id': reviewId,\n",
    "                              'stars': stars,\n",
    "                              'words': words}\n",
    "            #tagColls.insert(insertData)\n",
    "        bulkInsertData.append(insertData)\n",
    "        tagColls.insert_many(bulkInsertData)\n",
    "#worker(1, db, 0, 0)\n",
    "numWorkers = 2\n",
    "#print notTagged[:50]\n",
    "reviewCur = reviews.find({'review_id':{'$in':notTagged}})\n",
    "totalReviews = reviewCur.count()\n",
    "batchSize = totalReviews/numWorkers\n",
    "extra = totalReviews % numWorkers\n",
    "\n",
    "jobs = []\n",
    "for i in xrange(numWorkers):\n",
    "    left = 0\n",
    "    if i == (numWorkers - 1):\n",
    "        left = extra\n",
    "    p = multiprocessing.Process(target=worker, args=((i + 1), db, i * batchSize, batchSize + left))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "\n",
    "for j in jobs:\n",
    "    j.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews 453404\n",
      "10 20 30"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e99444292372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_json_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e99444292372>\u001b[0m in \u001b[0;36mcreate_json_array\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmandal/anaconda/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m    101\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 102\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmandal/anaconda/lib/python2.7/site-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def create_json_array():\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append(\"''\")\n",
    "    reviewColls = db.reviews\n",
    "    tagColls = db.tmptags\n",
    "    batchSize = 100\n",
    "    bulkInsertData = []\n",
    "    \n",
    "    \n",
    "    #for batch in xrange(0, limit, batchSize):\n",
    "    if True:\n",
    "        reviewCur = reviewColls.find({'review_id':{'$in':notTagged}})\n",
    "        count = 0\n",
    "        totalReviews = reviewCur.count()\n",
    "        print \"Total reviews\", totalReviews\n",
    "        percentComplete = 0\n",
    "        tenPercentCount = totalReviews/10\n",
    "        for review in reviewCur:\n",
    "            count += 1\n",
    "            words = []\n",
    "            reviewText = review['text'].lower()\n",
    "            businessId = review['business_id']\n",
    "            userId = review['user_id']\n",
    "            date = review['date']\n",
    "            reviewId = review['review_id']\n",
    "            stars = review['stars']\n",
    "            sentences = nltk.sent_tokenize(reviewText)\n",
    "            for sentence in sentences:\n",
    "                tokens = nltk.word_tokenize(sentence)\n",
    "                tokens = [t for t in tokens if t not in punctuations]\n",
    "                text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                for word, tag in tagged_text:\n",
    "                    words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "            insertData = {'business_id': businessId,\n",
    "                              'user_id': userId,\n",
    "                              'date': date,\n",
    "                              'review_id': reviewId,\n",
    "                              'stars': stars,\n",
    "                              'words': words}\n",
    "            #tagColls.insert(insertData)\n",
    "            bulkInsertData.append(insertData)\n",
    "            if count == tenPercentCount:\n",
    "                percentComplete += 10\n",
    "                print percentComplete,\n",
    "                count = 0\n",
    "        dump = json.dumps(bulkInsertData)\n",
    "        return dump\n",
    "        \n",
    "dump = create_json_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-58f0f62021fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/derivedData/tokenized.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/yelp_academic_dataset_review.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mjsonData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mreviewId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "punctuations.append(\"''\")\n",
    "bulkInsertData = []\n",
    "tagColls = db.tmptags\n",
    "with open('./data/derivedData/tokenized.json', 'a') as fwrite:\n",
    "    with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "        for line in fread:\n",
    "            jsonData = json.loads(line)\n",
    "            reviewId = jsonData['review_id']\n",
    "            if reviewId in notTagged:\n",
    "                words = []\n",
    "                reviewText = jsonData['text'].lower()\n",
    "                businessId = jsonData['business_id']\n",
    "                userId = jsonData['user_id']\n",
    "                date = jsonData['date']\n",
    "                stars = jsonData['stars']\n",
    "                sentences = nltk.sent_tokenize(reviewText)\n",
    "                for sentence in sentences:\n",
    "                    tokens = nltk.word_tokenize(sentence)\n",
    "                    tokens = [t for t in tokens if t not in punctuations]\n",
    "                    text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                    tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                    for word, tag in tagged_text:\n",
    "                        words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "                insertData = {'business_id': businessId,\n",
    "                                  'user_id': userId,\n",
    "                                  'date': date,\n",
    "                                  'review_id': reviewId,\n",
    "                                  'stars': stars,\n",
    "                                  'words': words}\n",
    "                json.dump(insertData, fwrite)\n",
    "                fwrite.write('\\n')\n",
    "            #bulkInsertData.append(insertData)\n",
    "#tagColls.insert_many(bulkInsertData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTaggedIds():\n",
    "    taggedReviews = []\n",
    "    with open('./data/derivedData/tokenized.json', 'rb') as fread:\n",
    "        for line in fread:\n",
    "            jsonData = json.loads(line)\n",
    "            taggedReviews.append(jsonData['review_id'])\n",
    "    return taggedReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537223 453404\n"
     ]
    }
   ],
   "source": [
    "moreTagged = getTaggedIds()\n",
    "print len(tagged_reviews), len(notTagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def tagReviews(unTaggedIds, allReviews):\n",
    "    punctuations = list(string.punctuation)\n",
    "    punctuations.append(\"''\")\n",
    "    #bulkInsertData = []\n",
    "    #tagColls = db.tmptags\n",
    "    \n",
    "    with open('./data/derivedData/tokenized1.json', 'a') as fwrite:\n",
    "            print 'writing...'\n",
    "            for reviewId in unTaggedIds:\n",
    "                data = allReviews[reviewId]\n",
    "            #for reviewId, data in allReviews.iteritems():\n",
    "                if True:\n",
    "                    #print line\n",
    "                    words = []\n",
    "                    reviewText = data['text'].lower()\n",
    "                    businessId = data['business_id']\n",
    "                    userId = data['user_id']\n",
    "                    date = data['date']\n",
    "                    stars = data['stars']\n",
    "                    \n",
    "                    sentences = nltk.sent_tokenize(reviewText)\n",
    "                    #print sentences\n",
    "                    for sentence in sentences:\n",
    "                        tokens = nltk.word_tokenize(sentence)\n",
    "                        tokens = [t for t in tokens if t not in punctuations]\n",
    "                        text = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "                        tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "                        for word, tag in tagged_text:\n",
    "                            words.append({\"word\": word, \"pos\": tag})\n",
    "\n",
    "                    insertData = {'business_id': businessId,\n",
    "                                      'user_id': userId,\n",
    "                                      'date': date,\n",
    "                                      'review_id': reviewId,\n",
    "                                      'stars': stars,\n",
    "                                      'words': words}\n",
    "                    json.dump(insertData, fwrite)\n",
    "                    fwrite.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453404 335869\n",
      "Read all reviews\n"
     ]
    }
   ],
   "source": [
    "unTaggedIds = list(set(notTagged) - set(moreTagged))\n",
    "print len(notTagged), len(unTaggedIds)\n",
    "allReviews = {}\n",
    "\n",
    "if True:\n",
    "    with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "        for line in fread:\n",
    "                \n",
    "            jsonData = json.loads(line)\n",
    "            reviewId = jsonData['review_id']\n",
    "            reviewText = jsonData['text'].lower()\n",
    "            businessId = jsonData['business_id']\n",
    "            userId = jsonData['user_id']\n",
    "            date = jsonData['date']\n",
    "            stars = jsonData['stars']\n",
    "            allReviews[reviewId] = {'text': reviewText,\n",
    "                                    'business_id': businessId,\n",
    "                                    'user_id': userId,\n",
    "                                    'date': date,\n",
    "                                    'stars': stars}\n",
    "                    \n",
    "    print 'Read all reviews'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569264\n",
      "writing...\n"
     ]
    }
   ],
   "source": [
    "print len(allReviews)\n",
    "tagReviews(unTaggedIds, allReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x3034a6a50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_colls = db.tags\n",
    "bulk_insert = []\n",
    "with open('./data/derivedData/tokenized.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        line = line.strip()\n",
    "        json_data = json.loads(line)\n",
    "        bulk_insert.append(json_data)\n",
    "tag_colls.insert_many(bulk_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.398188114\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "corpus_coll = db.corpus\n",
    "tags = db.tags.find()\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "for tag in tags:\n",
    "    nouns = []\n",
    "    words = [word for word in tag[\"words\"] if word[\"pos\"] in [\"NN\", \"NNS\"]]\n",
    "\n",
    "    for word in words:\n",
    "        nouns.append(lm.lemmatize(word[\"word\"]))\n",
    "\n",
    "    corpus_coll.insert({\n",
    "        \"review_id\": tag[\"review_id\"],\n",
    "        \"business_id\": tag[\"business_id\"],\n",
    "        \"words\": nouns\n",
    "    })\n",
    "\n",
    "print time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Dictionary\n",
    "from gensim import corpora\n",
    "path_to_dict = './data/dictionary'\n",
    "corpus_col = db.corpus\n",
    "corpus_cur = corpus_col.find()\n",
    "\n",
    "dictionary = corpora.Dictionary([corpus['words'] for corpus in corpus_cur])\n",
    "dictionary.filter_extremes(keep_n=10000)\n",
    "dictionary.compactify()\n",
    "corpora.Dictionary.save(dictionary, path_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReviewBleiCorpus at 0x3943751d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Blei corpus for LDA\n",
    "class ReviewBleiCorpus(object):\n",
    "    def __init__(self, corpus_cur, dictionary, path_to_corpus):\n",
    "        self.corpus_cur = corpus_cur\n",
    "        self.dictionary = dictionary\n",
    "        self.path_to_corpus = path_to_corpus\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for corpus in self.corpus_cur:\n",
    "            yield dictionary.doc2bow(corpus['words'])\n",
    "    \n",
    "    def save(self):\n",
    "        corpora.BleiCorpus.serialize(self.path_to_corpus, self, id2word=self.dictionary)\n",
    "        return self\n",
    "\n",
    "path_to_corpus = './data/bleicorpus'\n",
    "corpus_cur.rewind()\n",
    "review_corpus = ReviewBleiCorpus(corpus_cur, dictionary, path_to_corpus)\n",
    "review_corpus.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create LDA Model and serialize it\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "path_to_lda_model = './model/lda_model_multicore'\n",
    "corpus = corpora.BleiCorpus(path_to_corpus)\n",
    "lda = LdaMulticore(corpus, num_topics=50, id2word=dictionary)\n",
    "lda.save(path_to_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: 0.079*food + 0.042*service + 0.040*place + 0.034*great + 0.026*staff + 0.019*awesome + 0.014*salad + 0.014*vegan + 0.014*menu + 0.013*option\n",
      "#1: 0.025*wine + 0.018*course + 0.017*restaurant + 0.016*meal + 0.015*dish + 0.014*dessert + 0.013*menu + 0.012*food + 0.012*service + 0.010*dinner\n",
      "#2: 0.042*bar + 0.041*place + 0.039*breakfast + 0.028*game + 0.018*tv + 0.015*sport + 0.013*drink + 0.012*egg + 0.012*watch + 0.011*morning\n",
      "#3: 0.031*service + 0.029*waitress + 0.028*food + 0.025*table + 0.021*time + 0.020*manager + 0.016*server + 0.014*experience + 0.014*people + 0.013*party\n",
      "#4: 0.029*server + 0.016*dinner + 0.016*food + 0.016*meal + 0.015*waiter + 0.015*glass + 0.014*reservation + 0.014*room + 0.013*birthday + 0.012*husband\n",
      "#5: 0.025*food + 0.018*place + 0.011*l + 0.010*hawaii + 0.010*hawaiian + 0.007*aloha + 0.007*sloppy + 0.006*menu + 0.006*sauce + 0.006*luke\n",
      "#6: 0.127*steak + 0.028*medium + 0.020*rare + 0.017*side + 0.016*time + 0.016*filet + 0.015*steakhouse + 0.014*place + 0.014*service + 0.013*meat\n",
      "#7: 0.052*lot + 0.033*place + 0.033*parking + 0.018*food + 0.017*area + 0.013*park + 0.011*donut + 0.011*nice + 0.010*lunch + 0.010*street\n",
      "#8: 0.046*minute + 0.043*time + 0.039*food + 0.027*drink + 0.024*service + 0.021*wait + 0.020*waiter + 0.020*order + 0.017*friend + 0.016*place\n",
      "#9: 0.056*shrimp + 0.029*sauce + 0.015*egg + 0.011*time + 0.011*flavor + 0.010*menu + 0.010*item + 0.010*crab + 0.010*dish + 0.008*spicy\n",
      "#10: 0.213*pizza + 0.025*place + 0.019*crust + 0.015*sauce + 0.014*topping + 0.013*cheese + 0.010*time + 0.010*delivery + 0.010*pepperoni + 0.009*sausage\n",
      "#11: 0.031*sauce + 0.021*cake + 0.015*dessert + 0.015*chocolate + 0.014*flavor + 0.012*bread + 0.007*cream + 0.007*cheese + 0.007*sweet + 0.007*beef\n",
      "#12: 0.055*order + 0.043*food + 0.031*time + 0.030*customer + 0.024*service + 0.020*place + 0.015*get + 0.012*guy + 0.012*employee + 0.011*people\n",
      "#13: 0.114*hour + 0.096*happy + 0.026*wine + 0.024*special + 0.023*price + 0.021*place + 0.020*menu + 0.015*food + 0.014*deal + 0.013*service\n",
      "#14: 0.102*food + 0.055*service + 0.051*place + 0.036*kid + 0.028*price + 0.018*portion + 0.015*bland + 0.014*yum + 0.012*meal + 0.012*con\n",
      "#15: 0.082*le + 0.047*la + 0.037*et + 0.026*un + 0.019*pa + 0.017*pour + 0.017*est + 0.016*une + 0.016*je + 0.016*que\n",
      "#16: 0.052*pasta + 0.019*food + 0.018*meatball + 0.017*sauce + 0.016*restaurant + 0.016*drink + 0.014*place + 0.014*martini + 0.014*dish + 0.014*appetizer\n",
      "#17: 0.042*oyster + 0.020*http + 0.019*creme + 0.015*seafood + 0.015*dinner + 0.014*brulee + 0.013*dessert + 0.011*time + 0.011*sprout + 0.010*appetizer\n",
      "#18: 0.065*coffee + 0.023*cafe + 0.015*place + 0.013*store + 0.013*pastry + 0.012*cup + 0.012*shop + 0.009*time + 0.009*menu + 0.008*thing\n",
      "#19: 0.093*place + 0.034*food + 0.022*music + 0.021*night + 0.012*friend + 0.012*fun + 0.011*decor + 0.011*cool + 0.011*drink + 0.010*people\n",
      "#20: 0.049*sum + 0.040*dim + 0.023*cart + 0.017*place + 0.013*food + 0.012*dish + 0.010*lasagna + 0.010*time + 0.009*sauce + 0.009*restaurant\n",
      "#21: 0.081*salad + 0.040*cheese + 0.023*waffle + 0.023*mac + 0.019*lobster + 0.014*chicken + 0.013*bread + 0.012*food + 0.011*side + 0.010*place\n",
      "#22: 0.028*egg + 0.023*breakfast + 0.021*pancake + 0.020*toast + 0.019*french + 0.019*butter + 0.017*bacon + 0.014*place + 0.010*biscuit + 0.009*food\n",
      "#23: 0.038*ramen + 0.014*line + 0.014*truck + 0.014*place + 0.012*time + 0.011*review + 0.009*market + 0.008*food + 0.007*pork + 0.006*way\n",
      "#24: 0.134*burger + 0.094*fry + 0.022*onion + 0.016*place + 0.014*bun + 0.012*cheese + 0.010*time + 0.010*order + 0.010*ring + 0.009*bacon\n",
      "#25: 0.062*bar + 0.051*drink + 0.026*food + 0.025*bartender + 0.021*cocktail + 0.020*night + 0.017*service + 0.014*place + 0.014*patio + 0.014*menu\n",
      "#26: 0.090*buffet + 0.037*food + 0.023*crab + 0.019*line + 0.017*station + 0.017*dessert + 0.017*leg + 0.017*selection + 0.015*vega + 0.014*price\n",
      "#27: 0.119*sandwich + 0.025*place + 0.022*bread + 0.019*lunch + 0.015*sub + 0.014*bagel + 0.013*turkey + 0.013*time + 0.010*get + 0.010*deli\n",
      "#28: 0.099*wing + 0.029*sauce + 0.027*chicken + 0.017*place + 0.016*buffalo + 0.014*food + 0.013*time + 0.012*get + 0.011*ranch + 0.010*order\n",
      "#29: 0.036*brunch + 0.028*food + 0.026*time + 0.024*place + 0.022*juice + 0.022*service + 0.017*star + 0.017*bloody + 0.016*patio + 0.016*mary\n",
      "#30: 0.065*taco + 0.030*chip + 0.029*mexican + 0.026*salsa + 0.025*food + 0.021*place + 0.020*bean + 0.019*burrito + 0.016*margarita + 0.016*tortilla\n",
      "#31: 0.084*cream + 0.083*ice + 0.030*crepe + 0.017*dessert + 0.014*place + 0.010*time + 0.009*get + 0.009*chocolate + 0.009*strawberry + 0.008*vanilla\n",
      "#32: 0.018*time + 0.018*fiance + 0.014*ramsay + 0.014*muffin + 0.013*bf + 0.013*gordon + 0.013*ny + 0.011*service + 0.010*macaroon + 0.010*wellington\n",
      "#33: 0.050*restaurant + 0.042*vega + 0.031*strip + 0.031*food + 0.017*place + 0.015*service + 0.014*la + 0.011*experience + 0.010*price + 0.010*mall\n",
      "#34: 0.020*place + 0.016*get + 0.016*airport + 0.014*food + 0.009*lunch + 0.007*service + 0.006*island + 0.005*meal + 0.005*order + 0.005*flight\n",
      "#35: 0.054*excellent + 0.038*hash + 0.032*egg + 0.030*brown + 0.029*service + 0.022*food + 0.013*side + 0.013*staff + 0.011*place + 0.010*beef\n",
      "#36: 0.033*menu + 0.024*time + 0.024*day + 0.020*diner + 0.019*coupon + 0.016*food + 0.012*place + 0.012*omelet + 0.011*order + 0.008*online\n",
      "#37: 0.060*thai + 0.036*food + 0.032*curry + 0.028*lunch + 0.027*spicy + 0.023*place + 0.021*tea + 0.019*dish + 0.017*chicken + 0.015*time\n",
      "#38: 0.102*star + 0.055*dog + 0.050*food + 0.027*place + 0.022*hot + 0.019*chili + 0.012*service + 0.010*review + 0.010*get + 0.010*'m\n",
      "#39: 0.077*pie + 0.018*mahi + 0.015*place + 0.009*pot + 0.009*food + 0.008*john + 0.008*inch + 0.007*thing + 0.006*meal + 0.006*get\n",
      "#40: 0.106*bbq + 0.051*pork + 0.033*brisket + 0.017*da + 0.017*sauce + 0.016*meat + 0.015*und + 0.013*grit + 0.013*e + 0.012*side\n",
      "#41: 0.036*food + 0.018*place + 0.015*room + 0.013*restaurant + 0.009*wall + 0.009*get + 0.008*'m + 0.007*way + 0.007*bay + 0.006*blah\n",
      "#42: 0.078*food + 0.033*place + 0.030*time + 0.026*restaurant + 0.026*location + 0.025*quality + 0.022*service + 0.022*year + 0.017*family + 0.016*price\n",
      "#43: 0.099*place + 0.056*food + 0.041*time + 0.019*service + 0.016*thing + 0.014*try + 0.011*miss + 0.010*eat + 0.009*year + 0.009*town\n",
      "#44: 0.035*soup + 0.035*rice + 0.030*noodle + 0.020*beef + 0.019*place + 0.019*pho + 0.019*bowl + 0.018*dish + 0.018*sauce + 0.017*pork\n",
      "#45: 0.029*get + 0.022*place + 0.018*cheap + 0.015*food + 0.012*cash + 0.011*na + 0.011*'m + 0.008*drunk + 0.008*lol + 0.008*time\n",
      "#46: 0.103*sushi + 0.085*roll + 0.034*place + 0.018*tuna + 0.013*service + 0.013*time + 0.012*chef + 0.012*food + 0.011*sashimi + 0.010*fish\n",
      "#47: 0.067*slice + 0.030*pizza + 0.019*place + 0.018*mozzarella + 0.013*time + 0.010*sauce + 0.010*lunch + 0.009*poke + 0.009*salad + 0.009*try\n",
      "#48: 0.172*beer + 0.041*selection + 0.026*bar + 0.019*place + 0.015*pub + 0.015*tap + 0.010*pretzel + 0.010*food + 0.009*draft + 0.007*brew\n",
      "#49: 0.048*rib + 0.025*food + 0.024*place + 0.021*pita + 0.021*gyro + 0.019*meat + 0.018*greek + 0.018*salad + 0.017*lamb + 0.013*wrap\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = \"./data/dictionary\"\n",
    "corpus_path = \"./data/bleicorpus\"\n",
    "lda_num_topics = 50\n",
    "lda_model_path = \"./model/lda_model_multicore\"\n",
    "\n",
    "dictionary = corpora.Dictionary.load(dictionary_path)\n",
    "corpus = corpora.BleiCorpus(corpus_path)\n",
    "lda = LdaModel.load(lda_model_path)\n",
    "\n",
    "topic_dict = {}\n",
    "i = 0\n",
    "for topic in lda.show_topics(num_topics=lda_num_topics):\n",
    "    print '#' + str(i) + ': ' + topic\n",
    "    topic_dict[i] = {'scores': topic}\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "class TopicPrediction(object):\n",
    "    \n",
    "    def __init__(self, path_to_dictionary, path_to_model):\n",
    "        \n",
    "        self.path_to_dictionary = path_to_dictionary\n",
    "        self.path_to_model = path_to_model\n",
    "        self.dictionary = corpora.Dictionary.load(self.path_to_dictionary)\n",
    "        self.lda_model = LdaModel.load(self.path_to_model)\n",
    "        \n",
    "    def extract_pos(self, review_text):\n",
    "        stpwords = stopwords.words('english')\n",
    "        \n",
    "        punctuations = list(string.punctuation)\n",
    "        punctuations.append(\"''\")\n",
    "        \n",
    "        words = []\n",
    "        sentences = nltk.sent_tokenize(review_text)\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            tokens = [t for t in tokens if t not in punctuations]\n",
    "            text = [word for word in tokens if word not in stpwords]\n",
    "\n",
    "            tagged_text = nltk.pos_tag(text)\n",
    "\n",
    "            for word, tag in tagged_text:\n",
    "                words.append({\"word\": word, \"pos\": tag})\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def extract_lemmatized_nouns(self, review_text):\n",
    "        words = self.extract_pos(review_text)\n",
    "        lm = WordNetLemmatizer()\n",
    "        nouns = []\n",
    "        for word in words:\n",
    "            if word[\"pos\"] in [\"NN\", \"NNS\"]:\n",
    "                nouns.append(lm.lemmatize(word[\"word\"]))\n",
    "\n",
    "        return nouns\n",
    "    \n",
    "    def run(self, review_text):\n",
    "        nouns = self.extract_lemmatized_nouns(review_text)\n",
    "        bow = self.dictionary.doc2bow(nouns)\n",
    "        lda = self.lda_model[bow]\n",
    "\n",
    "        return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(37, 0.53049847545467033), (46, 0.12784771980340387), (44, 0.11487964532631743), (15, 0.080631566193878124), (48, 0.076911823990961276)]\n",
      "{'scores': u'0.060*thai + 0.036*food + 0.032*curry + 0.028*lunch + 0.027*spicy + 0.023*place + 0.021*tea + 0.019*dish + 0.017*chicken + 0.015*time'}\n"
     ]
    }
   ],
   "source": [
    "review = '''\n",
    "        Flavorful burritos, sandwiches, tacos and quesadillas!!! Everything is so fresh and ready quickly. I have to compare it to Chipotle because everyone knows Chipotle! I'd say it's just as good for sure, but the SALSA VERDE makes it special. What the heck is salsa verde and why can't I add it to everything? My fav is getting two tacos, side of ride with salsa verde all over it! \n",
    "Also, the girls that work here are nice and absolutely gorgeous! Chicas bonitas!\n",
    "'''\n",
    "\n",
    "review = '''\n",
    "I love this place and eat here at least once a week. Their pad si yew, pad Thai, dumplings, Siam rolls, and their curries are their best dishes. Especially the curry. You can't go wrong at Thai moon, so I suggest you go and enjoy the food!\n",
    "'''\n",
    "topic_prediction = TopicPrediction(path_to_dict, path_to_lda_model)\n",
    "predictions = topic_prediction.run(review)\n",
    "predictions = sorted(predictions, key=lambda tup: -tup[1])\n",
    "print predictions\n",
    "print topic_dict[predictions[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'scores': u'0.079*food + 0.042*service + 0.040*place + 0.034*great + 0.026*staff + 0.019*awesome + 0.014*salad + 0.014*vegan + 0.014*menu + 0.013*option'}\n",
      "1 {'scores': u'0.025*wine + 0.018*course + 0.017*restaurant + 0.016*meal + 0.015*dish + 0.014*dessert + 0.013*menu + 0.012*food + 0.012*service + 0.010*dinner'}\n",
      "2 {'scores': u'0.042*bar + 0.041*place + 0.039*breakfast + 0.028*game + 0.018*tv + 0.015*sport + 0.013*drink + 0.012*egg + 0.012*watch + 0.011*morning'}\n",
      "3 {'scores': u'0.031*service + 0.029*waitress + 0.028*food + 0.025*table + 0.021*time + 0.020*manager + 0.016*server + 0.014*experience + 0.014*people + 0.013*party'}\n",
      "4 {'scores': u'0.029*server + 0.016*dinner + 0.016*food + 0.016*meal + 0.015*waiter + 0.015*glass + 0.014*reservation + 0.014*room + 0.013*birthday + 0.012*husband'}\n",
      "5 {'scores': u'0.025*food + 0.018*place + 0.011*l + 0.010*hawaii + 0.010*hawaiian + 0.007*aloha + 0.007*sloppy + 0.006*menu + 0.006*sauce + 0.006*luke'}\n",
      "6 {'scores': u'0.127*steak + 0.028*medium + 0.020*rare + 0.017*side + 0.016*time + 0.016*filet + 0.015*steakhouse + 0.014*place + 0.014*service + 0.013*meat'}\n",
      "7 {'scores': u'0.052*lot + 0.033*place + 0.033*parking + 0.018*food + 0.017*area + 0.013*park + 0.011*donut + 0.011*nice + 0.010*lunch + 0.010*street'}\n",
      "8 {'scores': u'0.046*minute + 0.043*time + 0.039*food + 0.027*drink + 0.024*service + 0.021*wait + 0.020*waiter + 0.020*order + 0.017*friend + 0.016*place'}\n",
      "9 {'scores': u'0.056*shrimp + 0.029*sauce + 0.015*egg + 0.011*time + 0.011*flavor + 0.010*menu + 0.010*item + 0.010*crab + 0.010*dish + 0.008*spicy'}\n",
      "10 {'scores': u'0.213*pizza + 0.025*place + 0.019*crust + 0.015*sauce + 0.014*topping + 0.013*cheese + 0.010*time + 0.010*delivery + 0.010*pepperoni + 0.009*sausage'}\n",
      "11 {'scores': u'0.031*sauce + 0.021*cake + 0.015*dessert + 0.015*chocolate + 0.014*flavor + 0.012*bread + 0.007*cream + 0.007*cheese + 0.007*sweet + 0.007*beef'}\n",
      "12 {'scores': u'0.055*order + 0.043*food + 0.031*time + 0.030*customer + 0.024*service + 0.020*place + 0.015*get + 0.012*guy + 0.012*employee + 0.011*people'}\n",
      "13 {'scores': u'0.114*hour + 0.096*happy + 0.026*wine + 0.024*special + 0.023*price + 0.021*place + 0.020*menu + 0.015*food + 0.014*deal + 0.013*service'}\n",
      "14 {'scores': u'0.102*food + 0.055*service + 0.051*place + 0.036*kid + 0.028*price + 0.018*portion + 0.015*bland + 0.014*yum + 0.012*meal + 0.012*con'}\n",
      "15 {'scores': u'0.082*le + 0.047*la + 0.037*et + 0.026*un + 0.019*pa + 0.017*pour + 0.017*est + 0.016*une + 0.016*je + 0.016*que'}\n",
      "16 {'scores': u'0.052*pasta + 0.019*food + 0.018*meatball + 0.017*sauce + 0.016*restaurant + 0.016*drink + 0.014*place + 0.014*martini + 0.014*dish + 0.014*appetizer'}\n",
      "17 {'scores': u'0.042*oyster + 0.020*http + 0.019*creme + 0.015*seafood + 0.015*dinner + 0.014*brulee + 0.013*dessert + 0.011*time + 0.011*sprout + 0.010*appetizer'}\n",
      "18 {'scores': u'0.065*coffee + 0.023*cafe + 0.015*place + 0.013*store + 0.013*pastry + 0.012*cup + 0.012*shop + 0.009*time + 0.009*menu + 0.008*thing'}\n",
      "19 {'scores': u'0.093*place + 0.034*food + 0.022*music + 0.021*night + 0.012*friend + 0.012*fun + 0.011*decor + 0.011*cool + 0.011*drink + 0.010*people'}\n",
      "20 {'scores': u'0.049*sum + 0.040*dim + 0.023*cart + 0.017*place + 0.013*food + 0.012*dish + 0.010*lasagna + 0.010*time + 0.009*sauce + 0.009*restaurant'}\n",
      "21 {'scores': u'0.081*salad + 0.040*cheese + 0.023*waffle + 0.023*mac + 0.019*lobster + 0.014*chicken + 0.013*bread + 0.012*food + 0.011*side + 0.010*place'}\n",
      "22 {'scores': u'0.028*egg + 0.023*breakfast + 0.021*pancake + 0.020*toast + 0.019*french + 0.019*butter + 0.017*bacon + 0.014*place + 0.010*biscuit + 0.009*food'}\n",
      "23 {'scores': u'0.038*ramen + 0.014*line + 0.014*truck + 0.014*place + 0.012*time + 0.011*review + 0.009*market + 0.008*food + 0.007*pork + 0.006*way'}\n",
      "24 {'scores': u'0.134*burger + 0.094*fry + 0.022*onion + 0.016*place + 0.014*bun + 0.012*cheese + 0.010*time + 0.010*order + 0.010*ring + 0.009*bacon'}\n",
      "25 {'scores': u'0.062*bar + 0.051*drink + 0.026*food + 0.025*bartender + 0.021*cocktail + 0.020*night + 0.017*service + 0.014*place + 0.014*patio + 0.014*menu'}\n",
      "26 {'scores': u'0.090*buffet + 0.037*food + 0.023*crab + 0.019*line + 0.017*station + 0.017*dessert + 0.017*leg + 0.017*selection + 0.015*vega + 0.014*price'}\n",
      "27 {'scores': u'0.119*sandwich + 0.025*place + 0.022*bread + 0.019*lunch + 0.015*sub + 0.014*bagel + 0.013*turkey + 0.013*time + 0.010*get + 0.010*deli'}\n",
      "28 {'scores': u'0.099*wing + 0.029*sauce + 0.027*chicken + 0.017*place + 0.016*buffalo + 0.014*food + 0.013*time + 0.012*get + 0.011*ranch + 0.010*order'}\n",
      "29 {'scores': u'0.036*brunch + 0.028*food + 0.026*time + 0.024*place + 0.022*juice + 0.022*service + 0.017*star + 0.017*bloody + 0.016*patio + 0.016*mary'}\n",
      "30 {'scores': u'0.065*taco + 0.030*chip + 0.029*mexican + 0.026*salsa + 0.025*food + 0.021*place + 0.020*bean + 0.019*burrito + 0.016*margarita + 0.016*tortilla'}\n",
      "31 {'scores': u'0.084*cream + 0.083*ice + 0.030*crepe + 0.017*dessert + 0.014*place + 0.010*time + 0.009*get + 0.009*chocolate + 0.009*strawberry + 0.008*vanilla'}\n",
      "32 {'scores': u'0.018*time + 0.018*fiance + 0.014*ramsay + 0.014*muffin + 0.013*bf + 0.013*gordon + 0.013*ny + 0.011*service + 0.010*macaroon + 0.010*wellington'}\n",
      "33 {'scores': u'0.050*restaurant + 0.042*vega + 0.031*strip + 0.031*food + 0.017*place + 0.015*service + 0.014*la + 0.011*experience + 0.010*price + 0.010*mall'}\n",
      "34 {'scores': u'0.020*place + 0.016*get + 0.016*airport + 0.014*food + 0.009*lunch + 0.007*service + 0.006*island + 0.005*meal + 0.005*order + 0.005*flight'}\n",
      "35 {'scores': u'0.054*excellent + 0.038*hash + 0.032*egg + 0.030*brown + 0.029*service + 0.022*food + 0.013*side + 0.013*staff + 0.011*place + 0.010*beef'}\n",
      "36 {'scores': u'0.033*menu + 0.024*time + 0.024*day + 0.020*diner + 0.019*coupon + 0.016*food + 0.012*place + 0.012*omelet + 0.011*order + 0.008*online'}\n",
      "37 {'scores': u'0.060*thai + 0.036*food + 0.032*curry + 0.028*lunch + 0.027*spicy + 0.023*place + 0.021*tea + 0.019*dish + 0.017*chicken + 0.015*time'}\n",
      "38 {'scores': u\"0.102*star + 0.055*dog + 0.050*food + 0.027*place + 0.022*hot + 0.019*chili + 0.012*service + 0.010*review + 0.010*get + 0.010*'m\"}\n",
      "39 {'scores': u'0.077*pie + 0.018*mahi + 0.015*place + 0.009*pot + 0.009*food + 0.008*john + 0.008*inch + 0.007*thing + 0.006*meal + 0.006*get'}\n",
      "40 {'scores': u'0.106*bbq + 0.051*pork + 0.033*brisket + 0.017*da + 0.017*sauce + 0.016*meat + 0.015*und + 0.013*grit + 0.013*e + 0.012*side'}\n",
      "41 {'scores': u\"0.036*food + 0.018*place + 0.015*room + 0.013*restaurant + 0.009*wall + 0.009*get + 0.008*'m + 0.007*way + 0.007*bay + 0.006*blah\"}\n",
      "42 {'scores': u'0.078*food + 0.033*place + 0.030*time + 0.026*restaurant + 0.026*location + 0.025*quality + 0.022*service + 0.022*year + 0.017*family + 0.016*price'}\n",
      "43 {'scores': u'0.099*place + 0.056*food + 0.041*time + 0.019*service + 0.016*thing + 0.014*try + 0.011*miss + 0.010*eat + 0.009*year + 0.009*town'}\n",
      "44 {'scores': u'0.035*soup + 0.035*rice + 0.030*noodle + 0.020*beef + 0.019*place + 0.019*pho + 0.019*bowl + 0.018*dish + 0.018*sauce + 0.017*pork'}\n",
      "45 {'scores': u\"0.029*get + 0.022*place + 0.018*cheap + 0.015*food + 0.012*cash + 0.011*na + 0.011*'m + 0.008*drunk + 0.008*lol + 0.008*time\"}\n",
      "46 {'scores': u'0.103*sushi + 0.085*roll + 0.034*place + 0.018*tuna + 0.013*service + 0.013*time + 0.012*chef + 0.012*food + 0.011*sashimi + 0.010*fish'}\n",
      "47 {'scores': u'0.067*slice + 0.030*pizza + 0.019*place + 0.018*mozzarella + 0.013*time + 0.010*sauce + 0.010*lunch + 0.009*poke + 0.009*salad + 0.009*try'}\n",
      "48 {'scores': u'0.172*beer + 0.041*selection + 0.026*bar + 0.019*place + 0.015*pub + 0.015*tap + 0.010*pretzel + 0.010*food + 0.009*draft + 0.007*brew'}\n",
      "49 {'scores': u'0.048*rib + 0.025*food + 0.024*place + 0.021*pita + 0.021*gyro + 0.019*meat + 0.018*greek + 0.018*salad + 0.017*lamb + 0.013*wrap'}\n"
     ]
    }
   ],
   "source": [
    "for key, value in topic_dict.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = ['Las Vegas', 'Phoenix', 'Charlotte', 'Scottsdale']\n",
    "businessIds = []\n",
    "with open('./data/yelp_academic_dataset_business.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        #print jsonData\n",
    "        city = jsonData['city']\n",
    "        state = jsonData['state']\n",
    "        categories = jsonData['categories']\n",
    "        if 'Restaurants' in categories and city in cities:\n",
    "            businessIds.append(jsonData['business_id'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9543\n"
     ]
    }
   ],
   "source": [
    "print len(businessIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'star'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-c1918c31a568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbusinessId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0muserId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mstar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'star'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbusinessId\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbusinessIds\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstar\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0muserIds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'star'"
     ]
    }
   ],
   "source": [
    "userIds = []\n",
    "\n",
    "with open('./data/yelp_academic_dataset_review.json', 'rb') as fread:\n",
    "    for line in fread:\n",
    "        jsonData = json.loads(line)\n",
    "        businessId = jsonData['business_id']\n",
    "        userId = jsonData['user_id']\n",
    "        star = jsonData['star']\n",
    "        if businessId in businessIds and star > 3:\n",
    "            userIds.append(userId)\n",
    "            \n",
    "print len(userIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
